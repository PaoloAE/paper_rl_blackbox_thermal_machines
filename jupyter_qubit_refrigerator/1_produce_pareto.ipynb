{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superconducting Qubit Refrigerator: Produce Pareto Front\n",
    "Analyze the performance of cycles previously identified using ```0_train.ipynb```. See that Jupyter Notebook for more details on this system.\n",
    "\n",
    "This Jupyter Notebook allows to export text files with all the data to produce a Pareto front by evaluating the deterministic policies. It allows to view the training curves and to evaluate the performance of individual or collections of trainings (outputting the average reward, power, entropy production, efficiency, and coherence). \n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "import numpy as np\n",
    "import plotting\n",
    "import extra\n",
    "import sac_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Pareto-front txt file from the deterministic \n",
    "Produces the Pareto-front data evaluating the _deterministic_ policies learned during training. In particular, this code assumes that ```main_dir``` contains one folder for each repetition (5 in the manuscript), and each repetition folder contains a series of runs for various values of the trade-off weigth c. One text file with the Pareto-front data will be exported in each repetition folder. The evaluation of the _deterministic_ policy is performed doing ```24000``` steps on the environment and exponentially averaging the reward, the power, the negative entropy production and the coherence with ```gamma=0.99993```. It is possible to evaluate only the performance of the runs with ```parameters.txt``` that satisfy the conditions in ```conditions_dict``` (see next cell for an example). If ```conditions_dict``` is left empty, all runs are evaluated.\n",
    "\n",
    "This code will produce the exact Pareto front data shown in the manuscript if the ```paper_plot_data``` is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_dir = \"../paper_plot_data/qubit_refrigerator\"\n",
    "conditions_dict = {} \n",
    "\n",
    "#used to evaluate the efficiency\n",
    "def eff(pow,entropy,bh,bc):\n",
    "    return pow*bh/( pow*(bc-bh) + np.abs(entropy) )\n",
    "\n",
    "#loop over each repetition\n",
    "for run_dir_name in os.listdir(main_dir):\n",
    "    if os.path.isdir(os.path.join(main_dir,run_dir_name)):\n",
    "        run_dir = os.path.join(main_dir, run_dir_name)\n",
    "        det_data_list = []\n",
    "        log_dirs = extra.log_dirs_given_criteria(run_dir, conditions_dict)\n",
    "        #loop over each run in the current repetition\n",
    "        for (i, log_dir) in enumerate(log_dirs): \n",
    "            print(f\"Evaluating i = {i+1} of {len(log_dirs)} in repetition {run_dir_name}\")\n",
    "\n",
    "            #load the data\n",
    "            loaded_train = sac_multi.SacTrain()\n",
    "            loaded_train.load_train(log_dir, no_train=True)\n",
    "\n",
    "            #evaluate the model\n",
    "            det_eval = loaded_train.evaluate_current_policy(deterministic=True, steps=24000,\n",
    "                    gamma=0.99993,actions_to_plot=120, save_policy_to_file_name=\"det_policy.txt\",\n",
    "                    actions_ylim=None, suppress_show=False,dont_clear_output=False)\n",
    "\n",
    "            #append data\n",
    "            model_parameters = extra.params_from_log_dir(log_dir)\n",
    "            (a_val, bh, bc) = (model_parameters[\"A_END\"],float(model_parameters[\"b0\"]),\n",
    "                               float(model_parameters[\"b1\"]))\n",
    "            det_data_list.append([det_eval[1],det_eval[2], eff(det_eval[1],det_eval[2],bh,bc),\n",
    "                                  a_val, det_eval[0]])\n",
    "\n",
    "    #save the Pareto front txt file\n",
    "    np.savetxt(os.path.join(run_dir,\"det_pareto.txt\"), np.array(det_data_list, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and evaluate all cycles with given parameters\n",
    "The following code looks into the ```main_dir``` folder, and chooses all training folders where the ```params.txt``` has the parameters specificed in ```conditions_dict```, that can consist of multiple conditions. If it's an empty dictionary, all folders are considered. \n",
    "For each of these folder:\n",
    "- it prints the folder location, the last 6 running rewards that were logged, and the average of these 6 values;\n",
    "- it evaluates the performance of the _deterministic_ policy performing ```steps=24000``` steps on the environment exponentially averaging the reward, the power, and the negative entropy production with ```gamma=0.99993```. These 3 averages are plotted as a function of step to prove their convergence, and also the last ```100``` actions are plotted;\n",
    "- it prints the average reward, power, negative entropy, and efficiency computed with the deterministic policy as described above;\n",
    "- it prints the average coherence generated during the deterministic cycles as described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_dir = \"../paper_plot_data/qubit_refrigerator/1\"\n",
    "conditions_dict = {\"A_END\": \"0.8\"}\n",
    "\n",
    "#parameters to compute the efficiency from power and entropy production\n",
    "tc = 0.15\n",
    "th = 0.3\n",
    "\n",
    "#faster but less accurate evaluation using gamma=0.9999, steps=9000\n",
    "gamma = 0.99993\n",
    "steps = 24000\n",
    "\n",
    "log_dirs = extra.log_dirs_given_criteria(main_dir, conditions_dict)\n",
    "for (i, log_dir) in enumerate(log_dirs):\n",
    "    #print directory and index\n",
    "    print(f\"i: {i}, log_dir: {log_dir}\")\n",
    "    \n",
    "    #print last running rewards and their average\n",
    "    last_rewards, avg_reward = extra.ret_last_rewards_and_avg(log_dir,number_of_rewards=6)\n",
    "    print(f\"Last rewards: {last_rewards}\")\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    \n",
    "    #show the training plots\n",
    "    plotting.plot_sac_logs(log_dir,is_tri=False,plot_to_file_line=None,actions_per_log=1000,\n",
    "      suppress_show=False,save_plot=False,actions_ylim=None,actions_to_plot=100,dont_clear_output=True)\n",
    "    \n",
    "    #evaluate the deterministic policy \n",
    "    loaded_train = sac_multi.SacTrain()\n",
    "    loaded_train.load_train(log_dir, no_train=True)\n",
    "    eval_reward_det = loaded_train.evaluate_current_policy(deterministic=True, steps=steps,                                 \n",
    "                        gamma=gamma,actions_to_plot=100, actions_ylim=None, suppress_show=False,\n",
    "                        dont_clear_output=True)\n",
    "    print(f\"Det. Reward: {eval_reward_det[0]}\")\n",
    "    print(f\"Det. Power, Entropy, Efficiency: {eval_reward_det[1]}, {-eval_reward_det[2]},\" + \n",
    "              f\"{eval_reward_det[1]*tc/(eval_reward_det[1]*(th-tc) -eval_reward_det[2]*th*tc ) }\")\n",
    "    \n",
    "    #print the coherence if returned by the environment\n",
    "    if len(eval_reward_det)>3:\n",
    "        print(f\"Coherence: { eval_reward_det[3]}\")\n",
    "    \n",
    "    #break line\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
