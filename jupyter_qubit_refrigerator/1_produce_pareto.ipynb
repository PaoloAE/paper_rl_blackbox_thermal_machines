{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superconducting Qubit Refrigerator: Produce Pareto Front\n",
    "Analyze the performance of cycles previously identified using ```0_train.ipynb```. See that Jupyter Notebook for more details on this system.\n",
    "\n",
    "This Jupyter Notebook allows to export a text file with all the data to produce a Pareto front by evaluating the deterministic policies. It allows to view the training curves and to evaluate the performance of individual or collections of trainings (outputting the average reward, power, entropy production, efficiency, and coherence). \n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "import numpy as np\n",
    "import plotting\n",
    "import extra\n",
    "import sac_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Pareto-front txt file from the deterministic Policy\n",
    "Produces a txt file with all the data to plot the Pareto front by evaluating the _deterministic_ policies learned during training. It does this by evaluating the performance of all trainings contained in the folder ```main_dir```, such that the parameters in ```parameters.txt``` satisfy the conditions in ```conditions_dict``` (see next cell for an example). If ```conditions_dict``` is left empty, all folders are evaluated. The Pareto front data is exported to ```main_dir``` as ```det_pareto.txt```. The evaluation of the _deterministic_ policy is performed doing ```24000``` steps on the environment and exponentially averaging the reward, the power, and the negative entropy production with ```gamma=0.99993```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_dir = \"../paper_plot_data/qubit_refrigerator/pareto\"\n",
    "conditions_dict = {} \n",
    "\n",
    "#used to evaluate the efficiency\n",
    "def eff(pow,entropy,bh,bc):\n",
    "    return pow*bh/( pow*(bc-bh) + np.abs(entropy) )\n",
    "\n",
    "det_data_list = []\n",
    "log_dirs = extra.log_dirs_given_criteria(main_dir, conditions_dict)\n",
    "for (i, log_dir) in enumerate(log_dirs): \n",
    "    print(f\"Evaluating i = {i+1} of {len(log_dirs)}\")\n",
    "    \n",
    "    #load the data\n",
    "    loaded_train = sac_multi.SacTrain()\n",
    "    loaded_train.load_train(log_dir, no_train=True)\n",
    "    \n",
    "    #evaluate the model\n",
    "    det_eval = loaded_train.evaluate_current_policy(deterministic=True, steps=24000, gamma=0.99993,actions_to_plot=120,\n",
    "                    save_policy_to_file_name=\"det_policy.txt\",actions_ylim=None, suppress_show=False,dont_clear_output=False)\n",
    "    \n",
    "    #append data\n",
    "    model_parameters = extra.params_from_log_dir(log_dir)\n",
    "    (a_val, bh, bc) = (model_parameters[\"A_END\"],float(model_parameters[\"b0\"]),float(model_parameters[\"b1\"]))\n",
    "    det_data_list.append([det_eval[1],det_eval[2], eff(det_eval[1],det_eval[2],bh,bc), a_val, det_eval[0]])\n",
    "\n",
    "#save the Pareto front txt file\n",
    "np.savetxt(os.path.join(main_dir,\"det_pareto.txt\"), np.array(det_data_list, dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and evaluate all cycles with given parameters\n",
    "The following code looks into the ```main_dir``` folder, and chooses all training folders where the ```params.txt``` has the parameters specificed in ```conditions_dict```, that can consist of multiple conditions. If it's an empty dictionary, all folders are considered. \n",
    "For each of these folder:\n",
    "- it prints the folder location, the last 6 running rewards that were logged, and the average of these 6 values;\n",
    "- it evaluates the performance of the _deterministic_ policy performing ```steps=24000``` steps on the environment exponentially averaging the reward, the power, and the negative entropy production with ```gamma=0.99993```. These 3 averages are plotted as a function of step to prove their convergence, and also the last ```100``` actions are plotted;\n",
    "- it prints the average reward, power, negative entropy, and efficiency computed with the deterministic policy as described above;\n",
    "- it prints the average coherence generated during the deterministic cycles as described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_dir = \"../paper_plot_data/qubit_refrigerator/pareto\"\n",
    "conditions_dict = {\"A_END\": \"0.8\"}\n",
    "\n",
    "#parameters to compute the efficiency from power and entropy production\n",
    "tc = 0.15\n",
    "th = 0.3\n",
    "\n",
    "#faster but less accurate evaluation using gamma=0.9999, steps=9000\n",
    "gamma = 0.99993\n",
    "steps = 24000\n",
    "\n",
    "log_dirs = extra.log_dirs_given_criteria(main_dir, conditions_dict)\n",
    "for (i, log_dir) in enumerate(log_dirs):\n",
    "    #print directory and index\n",
    "    print(f\"i: {i}, log_dir: {log_dir}\")\n",
    "    \n",
    "    #print last running rewards and their average\n",
    "    last_rewards, avg_reward = extra.ret_last_rewards_and_avg(log_dir,number_of_rewards=6)\n",
    "    print(f\"Last rewards: {last_rewards}\")\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    \n",
    "    #show the training plots\n",
    "    plotting.plot_sac_logs(log_dir,is_tri=False,plot_to_file_line=None,actions_per_log=1000,\n",
    "      suppress_show=False,save_plot=False,actions_ylim=None,actions_to_plot=100,dont_clear_output=True)\n",
    "    \n",
    "    #evaluate the deterministic policy \n",
    "    loaded_train = sac_multi.SacTrain()\n",
    "    loaded_train.load_train(log_dir, no_train=True)\n",
    "    eval_reward_det = loaded_train.evaluate_current_policy(deterministic=True, steps=steps, gamma=gamma,actions_to_plot=100,\n",
    "                        actions_ylim=None, suppress_show=False,dont_clear_output=True)\n",
    "    print(f\"Det. Reward: {eval_reward_det[0]}\")\n",
    "    print(f\"Det. Power, Entropy, Efficiency: {eval_reward_det[1]}, {-eval_reward_det[2]},\" + \n",
    "              f\"{eval_reward_det[1]*tc/(eval_reward_det[1]*(th-tc) -eval_reward_det[2]*th*tc ) }\")\n",
    "    \n",
    "    #print the coherence if returned by the environment\n",
    "    if len(eval_reward_det)>3:\n",
    "        print(f\"Coherence: { eval_reward_det[3]}\")\n",
    "    \n",
    "    #break line\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
