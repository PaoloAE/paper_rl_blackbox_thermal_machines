{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superconducting Qubit Refrigerator: Train\n",
    "Optimize the cooling power of a refrigerator based on a superconducting qubit (see Results section of the manuscript or Refs. [1](https://doi.org/10.1103/PhysRevB.94.184503), [2](https://doi.org/10.1103/PhysRevB.100.085405) or [3](https://doi.org/10.1103/PhysRevB.100.035407)). The Hamiltonian of the system is:\n",
    "\\begin{equation}\n",
    "\t\\hat{H}[u(t)] = - E_0\\left[\\Delta \\hat{\\sigma}_x + u(t)\\hat{\\sigma}_z  \\right],\n",
    "\t\\label{eq:h_fridge}\n",
    "\\end{equation}\n",
    "where $\\hat{\\sigma}_x$ and $\\hat{\\sigma}_z$ are Pauli matrices, $E_0$ is a fixed energy scale, $\\Delta$ characterizes the minimum gap of the system, and $u(t)$ is our single continuous control parameter. In this setup the coupling to the bath is fixed, so we do not have the discrete action of choosing the bath.\n",
    "The coupling to the baths is described using the Lindblad master equation [see Eq. (52) of the manuscript]. The Lindblad operators and corresponding rates are gived by\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\t\\hat{A}^{(\\alpha)}_{+,u(t)} &= -i\\rvert e_{u(t)}\\rangle \n",
    "    \\langle g_{u(t)} \\rvert, &\n",
    "\t\\hat{A}^{(\\alpha)}_{-,u(t)} &= +i\\rvert g_{u(t)}\\rangle \\langle e_{u(t)}\\rvert,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\rvert g_{u(t)}\\rangle$ and $\\rvert e_{u(t)}\\rangle$ are, respectively, the instantaneous ground state and excited state of the qubit. The corresponding rates are given by $\\gamma^{(\\alpha)}_{\\pm,u(t)} = S_{\\alpha}[\\pm\\Delta \\epsilon_{u(t)}] $, where $\\Delta \\epsilon_{u(t)}$ is the instantaneous energy gap of the system, and\n",
    "\\begin{equation}\n",
    "\tS_\\alpha(\\Delta \\epsilon)= \\frac{g_{\\alpha}}{2} \\frac{1}{1+Q_\\alpha^2( \\Delta\\epsilon/\\omega_\\alpha - \\omega_\\alpha/\\Delta \\epsilon )^2 } \\frac{\\Delta \\epsilon}{e^{\\beta_\\alpha\\Delta\\epsilon}-1}\n",
    "\\end{equation}\n",
    "is the noise power spectrum of bath $\\alpha$. Here $\\omega_\\alpha$, $Q_\\alpha$ and $g_\\alpha$ are the base resonance frequency, quality factor and coupling strength of the resonant circuit acting as bath $\\alpha=\\text{H},\\text{C}$, and $\\beta_\\alpha$ is the inverse temperature of bath $\\alpha$.\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "import numpy as np\n",
    "import sac_multi\n",
    "import sac_multi_envs\n",
    "import extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new Training\n",
    "The following codes initiates a new training session. All training logs, parameters and saved states will be stored under the ```data``` folder, within a folder with the current date and time. \n",
    "- ```env_params``` is a dictionary with the environment parameters.\n",
    "- ```training_hyperparams``` is a dictionary with training hyperparameters.\n",
    "- ```log_info``` is a dictionary that specifices which quantities to log.\n",
    "\n",
    "The parameters below were used to produce Figs. 3-4 of the manuscript. The parameter ```a_end``` determines the final value of the weight c. One training for each value of the parameter ```a_end``` in the range $[0.4,1]$ must be performed to produce the points of Fig. 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final value of the weight c (here denoted with a) \n",
    "a_end = 0.6\n",
    "\n",
    "#other parameters\n",
    "e0 = 1.\n",
    "delta = 0.12\n",
    "dt = 0.9817477042468103 \n",
    "env_params = {\n",
    "    \"g0\": 1.,                                  #g of bath 0\n",
    "    \"g1\": 1.,                                  #g of bath 1\n",
    "    \"b0\": 1/0.3,                               #inverse temperature \\beta of bath 0\n",
    "    \"b1\": 1/0.15,                              #inverse temperature \\beta of bath 1\n",
    "    \"q0\": 4.,                                  #quality factor of bath 0\n",
    "    \"q1\": 4.,                                  #quality factor of bath 1\n",
    "    \"e0\": e0,                                  #E_0\n",
    "    \"delta\": delta,                            #\\Deta\n",
    "    \"w0\": 2.*e0*np.sqrt(delta**2 + 0.25),      #resonance frequency of bath 0\n",
    "    \"w1\": 2.*e0*delta,                         #resonance frequency of bath 1\n",
    "    \"min_u\": 0.,                               #minimum value of the control u\n",
    "    \"max_u\": 0.75,                             #maximum value of the control u\n",
    "    \"dt\": dt,                                  #time step \\Delta t\n",
    "    \"p_coeff\": 1.51*10**3,                     #the power is multiplied by this\n",
    "    \"entropy_coeff\": 27,                       #the entropy production is multiplied by this\n",
    "    \"state_steps\": 128                         #time-steps N defining the state\n",
    "} \n",
    "training_hyperparams = {\n",
    "    \"BATCH_SIZE\": 512,                         #batch size          \n",
    "    \"LR\": 0.0003,                              #learning rate\n",
    "    \"H_START\": 0.,                             #initial policy entropy\n",
    "    \"H_END\": -3.5,                             #final policy entropy\n",
    "    \"H_DECAY\": 440000,                         #exponential decay of policy entropy\n",
    "    \"A_START\": 1.,                             #initial value of weight c\n",
    "    \"A_END\": a_end,                            #final value of weight c\n",
    "    \"A_DECAY\": 20000,                          #sigmoid decay of weight c\n",
    "    \"A_MEAN\": 170000,                          #sigmoid mean of weight c\n",
    "    \"REPLAY_MEMORY_SIZE\": 280000,              #size of replay buffer\n",
    "    \"POLYAK\": 0.995,                           #polyak coefficient\n",
    "    \"LOG_STEPS\": 1000,                         #save logs and display training every num. steps\n",
    "    \"GAMMA\": 0.997,                            #RL discount factor\n",
    "    \"CHANNEL_SIZES\":(64,64,64,128,128,128,128),#channels per conv. block\n",
    "    \"PI_FC_SIZES\": (256,),                     #sizes of hidden layers for the policy\n",
    "    \"Q_FC_SIZES\": (256,256),                   #sizes of hidden layers for the value function\n",
    "    \"SAVE_STATE_STEPS\": 500000,                #save state of training every num. steps          \n",
    "    \"INITIAL_RANDOM_STEPS\": 5000,              #number of initial uniformly random steps\n",
    "    \"UPDATE_AFTER\": 1000,                      #start minimizing loss function after num. steps\n",
    "    \"UPDATE_EVERY\": 50,                        #performs this many updates every this many steps\n",
    "    \"USE_CUDA\": True,                          #use cuda for computation\n",
    "    \"ALPHA_RESET_VAL\": 1e-06                   #reset value for temp. alpha if it becomes negative\n",
    "}\n",
    "log_info = {\n",
    "    \"log_running_reward\": True,                #log running reward\n",
    "    \"log_running_loss\": True,                  #log running loss\n",
    "    \"log_actions\": True,                       #log chosen actions\n",
    "    \"log_running_multi_obj\": True,             #log running multi objectives\n",
    "    \"extra_str\": f\"_aend={a_end}\"              #string to append to training folder name\n",
    "}\n",
    "\n",
    "#Speeds up trainig, but disables profiling\n",
    "extra.enable_faster_training()\n",
    "\n",
    "#initialize training object\n",
    "train = sac_multi.SacTrain()\n",
    "train.initialize_new_train(sac_multi_envs.CoherentQubitFridgePowEntropy, env_params, training_hyperparams, log_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times. While training, the following running averages are plotted:\n",
    "- G: running average of the reward;\n",
    "- Obj 0: the first objective, i.e. the cooling power;\n",
    "- Obj 1: the second objective, i.e. the negative entropy production;\n",
    "- Q Runninng Loss;\n",
    "- Pi Running Loss;\n",
    "- alpha: the temperature parameter of the SAC method;\n",
    "- entropy: the average entropy of the policy;\n",
    "- c weight.\n",
    "\n",
    "At last, the action u taken in the last 400 steps are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.train(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the State\n",
    "The full state of the training session is saved every ```SAVE_STATE_STEPS``` steps. Run this command if you wish to manually save the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_full_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Training\n",
    "Any training session that was saved can be loaded specifying the training session folder in ```log_dir```. This will produce a new folder for logging with the current date-time. It is then possible to train the model for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/qubit_refrigerator/pareto/2022_01_23-22_05_04_aend=0.7/\"\n",
    "train = sac_multi.SacTrain()\n",
    "train.load_train(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
