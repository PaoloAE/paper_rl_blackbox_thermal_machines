{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Harmonic Oscillator Heat Engine: Train\n",
    "Optimize the output power of a heat engine based on a quantum harmonic oscillator (see Results section of the manuscript or [this](https://doi.org/10.1088/1367-2630/8/5/083) reference). The Hamiltonian of the system is:\n",
    "\\begin{equation}\n",
    "\t\\hat{H}[u(t)] = \\frac{1}{2m} \\hat{p}^2 + \\frac{1}{2}m (u(t)w_0)^2 \\hat{q}^2,\n",
    "\\end{equation}\n",
    "where $m$ is the mass of the system, $w_0$ is a fixed frequency and $\\hat{p}$ and $\\hat{q}$ are the momentum and position operators. The single continuous control parameter is $u(t)$. \n",
    "The coupling to the baths is described using the Lindblad master equation [see Eq. (52) of the manuscript]. The Lindblad operators and corresponding rates are gived by\n",
    "\\begin{align}\n",
    "\t\\hat{A}^{(\\alpha)}_{+,u(t)} &= \\hat{a}_{u(t)}^\\dagger, & \\gamma^{(\\alpha)}_{+,u(t)} &= \\Gamma_\\alpha \\,n(\\beta_\\alpha u(t)\\omega_0), \\\\\n",
    "    \\hat{A}^{(\\alpha)}_{-,u(t)} &= \\hat{a}_{u(t)}, & \\gamma^{(\\alpha)}_{-,u(t)} &= \\Gamma_\\alpha[1+ n(\\beta_\\alpha u(t) \\omega_0 )],\n",
    "\\end{align}\n",
    "where $\\hat{a}_{u(t)}=(1/\\sqrt{2})\\sqrt{m\\omega_0 u(t)}\\,\\hat{q} + i/\\sqrt{m\\omega_0 u(t)}\\,\\hat{p}$ and $\\hat{a}_{u(t)}^\\dagger$ are respectively the (control dependent) lowering and raising operators, $\\Gamma_\\alpha$ are constant rates, $n(x)=(\\exp(x)-1)^{-1}$ is the Bose-Einstein distribution and $\\beta_\\alpha$ is the inverse temperature of bath $\\alpha$.\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "import numpy as np\n",
    "import sac_tri\n",
    "import sac_tri_envs\n",
    "import extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new Training\n",
    "The following codes initiates a new training session. All training logs, parameters and saved states will be stored under the ```data``` folder, within a folder with the current date and time. \n",
    "- ```env_params``` is a dictionary with the environment parameters.\n",
    "- ```training_hyperparams``` is a dictionary with training hyperparameters.\n",
    "- ```log_info``` is a dictionary that specifices which quantities to log.\n",
    "\n",
    "The parameters below were used to produce Fig. 5 of the manuscript. The parameter ```a``` determines the value of the weight c. One training for each value of the parameter ```a``` in the range $[0.5,1]$ must be performed to produce the points of Fig. 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of the weight c (here denoted with a) \n",
    "a = 1.\n",
    "\n",
    "env_params = { \n",
    "    \"g0\": 0.6,                                  #\\Gamma of bath 0\n",
    "    \"g1\": 0.6,                                  #\\Gamma of bath 1\n",
    "    \"b0\": 1./4.98309,                           #inverse temperature \\beta of bath 0\n",
    "    \"b1\": 2.,                                   #inverse temperature \\beta of bath 1\n",
    "    \"w0\": 2.,                                   #\\omega_0\n",
    "    \"min_u\": 0.5,                               #minimum value of action u\n",
    "    \"max_u\": 0.99662,                           #maximum value of action u\n",
    "    \"dt\": 0.2,                                  #timestep \\Delta t\n",
    "    \"p_coeff\": 1/0.175,                         #the power is multiplied by this\n",
    "    \"entropy_coeff\": 1/0.525,                   #the entropy production is multiplied by this\n",
    "    \"state_steps\": 128,                         #time-steps N defining the state\n",
    "    \"min_temp_steps\": 25,                       #minimum num. of steps with each bath before penalty\n",
    "    \"discourage_coeff\": 1.4,                    #coefficient determining the penalty\n",
    "    \"a\": a                                      #value of weight c\n",
    "}\n",
    "training_hyperparams = {\n",
    "    \"BATCH_SIZE\": 512,                          #batch size\n",
    "    \"LR\": 0.0003,                               #learning rate\n",
    "    \"H_D_START\": np.log(3.),                    #initial discrete policy entropy\n",
    "    \"H_D_END\": 0.01,                            #final discrete policy entropy\n",
    "    \"H_D_DECAY\": 144000,                        #exponential decay of discrete policy entropy\n",
    "    \"H_C_START\": -0.72,                         #initial continuous policy entropy\n",
    "    \"H_C_END\": -3.5,                            #final continuous policy entropy\n",
    "    \"H_C_DECAY\": 144000,                        #exponential decay of continuous policy entropy\n",
    "    \"REPLAY_MEMORY_SIZE\": 160000,               #size of replay buffer\n",
    "    \"POLYAK\": 0.995,                            #polyak coefficient\n",
    "    \"LOG_STEPS\": 1000,                          #save logs and display training every num. steps\n",
    "    \"GAMMA\": 0.999,                             #RL discount factor\n",
    "    \"CHANNEL_SIZES\": (64,64,64,128,128,128,128),#channels per conv. block\n",
    "    \"PI_FC_SIZES\": (256,),                      #size of hidden layers for the policy\n",
    "    \"Q_FC_SIZES\": (256,128),                    #size of hidden layers for the value funcion\n",
    "    \"SAVE_STATE_STEPS\": 500000,                 #save state of training every num. steps\n",
    "    \"INITIAL_RANDOM_STEPS\": 5000,               #number of initial uniformly random steps\n",
    "    \"UPDATE_AFTER\": 1000,                       #start minimizing loss function after num. steps\n",
    "    \"UPDATE_EVERY\": 50,                         #performs this many updates every this many steps\n",
    "    \"USE_CUDA\": True,                           #use cuda for computation\n",
    "    \"ALPHA_RESET_VAL\": 1e-6                     #reset value for temp. alpha if it becomes negative\n",
    "}\n",
    "log_info = {\n",
    "    \"log_running_reward\": True,                 #log running reward\n",
    "    \"log_running_loss\": True,                   #log running loss\n",
    "    \"log_actions\": True,                        #log chosen actions\n",
    "    \"log_running_multi_obj\": True,              #log running multi objectives\n",
    "    \"extra_str\": f\"_a={a}\"                      #string to append to training folder name\n",
    "}\n",
    "\n",
    "#Speeds up trainig, but disables profiling\n",
    "extra.enable_faster_training()\n",
    "\n",
    "#initialize training object\n",
    "train = sac_tri.SacTrain()\n",
    "train.initialize_new_train(sac_tri_envs.HarmonicEnginePowEntropy, env_params, training_hyperparams, log_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.train(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the State\n",
    "The full state of the training session is saved every ```SAVE_STATE_STEPS``` steps. Run this command if you wish to manually save the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_full_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Training\n",
    "Any training session that was saved can be loaded specifying the training session folder in ```log_dir```. This will produce a new folder for logging with the current date-time. It is then possible to train the model for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/harmonic_engine/pareto/2022_01_15-03_44_43_a=0.6\"\n",
    "train = sac_tri.SacTrain()\n",
    "train.load_train(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
