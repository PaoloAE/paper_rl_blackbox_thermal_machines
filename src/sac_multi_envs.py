from __future__ import print_function
import sys
import os
sys.path.append(os.path.join('..','src'))
import numpy as np
from gym import spaces
import sac_multi_phys_envs

"""
This module contains gym.Env environments that can be trained using sac_multi.SacTrain. These envs only
have continuous actions. Multi stands for "multiobjective", meaning that these enviroments provide
two objectives to be optimized, and allow for a variable weight c. 

In these environments, the state is a numpy array of floats, containing the sequence of the last N actions.
Its shape is (act_dim, N), where act_dim is the size of the continuous action, and N is the number of 
actions defining the state.

These environments, besides being proper gym.Env, MUST satisfy these additional requirements:
    1) __init__ must accept a single dict with all the parameters necessary to define the environment.
    2) implement set_current_state(state). Functions that takes a state as input, and sets the environment to that 
    3) the step() function must take as input (action, a_weight), where action is the action, and a_weight is the
     value of the weight c (that indeed can vary during training). 
    4) the step() function must provide in the info_dict the key "multi_obj" corresponding to a np.array with the
     value of the two objectives being optimized.
    5) Optional: if we want to evaluate the average coherence generated by a cycle, the step() function should proving
    in the info_dict the key "coherence" corresponding to a float with the average coherence during the timestep.
            np.array([power,entropy], dtype=np.float32), "coherence": coherence}
"""

class CoherentQubitFridgePowEntropy(sac_multi_phys_envs.CoherentQubitFridgePowEntropyNonBlackbox):
    """
    Gym.Env representing a refrigerator based on a qubit where the sigma_x component is fixed, and the
    sigma_z prefactor is the only continuous controllable parameter. See 
    jupyter/superconducting_qubit_refrigerator.ipynb and the "Results" section of the manuscript
    for additional info. The equation for the evolution are derived from the Lindblad equation given in
    https://doi.org/10.1103/PhysRevB.100.035407
    The reward is a weighed average between the cooling power out of bath 1, and the negative entropy
    production. One must specify inverse temperatures such that b0 <= b1.
    This environment loads src/sac_envs.CoherentQubitFridgePowEntropyNonBlackbox, and is adapted to 
    be trained by sac_multi.SacTrain by only exposing as state the sequence of the last N actions, and 
    not the actual quantum state.

    The seq_state variable is a numpy array representing the state, i.e. the sequence of the last N actions
    as defined in the module doc string.

    Args:
        env_params is a dictionary that must contain the following: 
        "g0" (float): g of bath 0
        "g1" (float): g of bath 1
        "b0" (float): inverse temperature \beta of bath 0
        "b1" (float): inverse temperature \beta of bath 1
        "q0" (float): quality factor of bath 0
        "q1" (float): quality factor of bath 1
        "e0" (float): E_0
        "delta" (float): \Delta
        "w0" (float): resonance frequency of bath 0
        "w1" (float): resonance frequency of bath 1
        "min_u" (float): minimum value of action u
        "max_u" (float): maximum value of action u
        "dt" (float): timestep \Delta t
        "p_coeff" (float): the cooling power is multiplied by this factor
        "entropy_coeff" (float): the entropy production is multiplied by this factor
        "state_steps" (int): number if time-steps used to define the state
    """
    def __init__(self, env_params):
        super().__init__(env_params)

        #load the "state_steps" parameter
        self.state_steps = env_params["state_steps"]

        #get the dimension of the action space
        self.act_dim = self.action_space.shape[0]

        #initialize the state variable
        self.seq_state = np.zeros((self.act_dim,self.state_steps))

        #override the observation space with the new specifications
        self.observation_space = spaces.Box( 
            low = np.transpose(np.broadcast_to(self.action_space.low, (self.state_steps, self.act_dim))),
            high = np.transpose(np.broadcast_to(self.action_space.high, (self.state_steps,self.act_dim))) )
        
        #initialize the state with a sequence of random actions
        random_actions = self.observation_space.sample()   
        self.set_current_state(random_actions)

    def current_state(self):
        """
        Returns the current state as the type specificed by self.observation_space.
        By overriding it, super().step() returns the new state.
        """
    
        return self.seq_state

    def set_current_state(self, state):
        """ 
        Allows to set the current state of the environment. This function must be implemented in order
        for sac_multi.SacTrain.load_train() to properly load a saved training session.

        Args:
            state (type specificed by self.observation_space): state of the environment
        """
        self.apply_actions_array(state)

    def step(self, action, a_weight):
        """ Evolves the state for a timestep depending on the chosen action

        Args:
            action (type specificed by self.action_space): the action to perform on the environment
            a_weight (float): the value of the weight c

        Raises:
            Exception: action out of bound

        Returns:
            state(np.Array): new state after the step
            reward(float): the reward, i.e. the weighed average between cooling power and
                negative entropy production during the current timestep
            end(bool): whether the episode ended (these environments never end)
            info_dict: dictionary with the following two keys:
                "multi_obj"(np.array): array with power and negative entropy production
                "coherence"(float): coherence in instantaneous eigenstates in current timestep
        """

        #append the latest action to the state
        self.add_action_to_state(action)

        #call the step from inherited environment
        return super().step(action, a_weight)

    def apply_actions_array(self, actions_array):
        """
        Applies a sequence of actions to the environment.

        Args:
            actions_array (np.Array): an array of actions. First elements are applied first
        """
        for i in range(actions_array.shape[1]):
            #i pass a reference weight of 1, since it doesnt matter
            self.step(actions_array[:,i],1.)

    def add_action_to_state(self, action):
        """
        Adds an action to the state array that is the sequence of actions.

        Args:
            action (type specificed by self.action_space): the action to add 
        """

        self.seq_state = np.roll(self.seq_state,-1,axis=1)
        self.seq_state[:,-1] = action

